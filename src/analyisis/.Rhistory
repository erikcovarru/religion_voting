c_val <- c_grid[j]  # Get the current c value
# Correctly apply conditions across all simulations for this c
for (b in 1:B) {  # Loop over each simulation
Y_star_array[W == 1 & W_star_matrix[, b] == 0, b, j] <- Y[W == 1 & W_star_matrix[, b] == 0] - c_val
Y_star_array[W == 0 & W_star_matrix[, b] == 1, b, j] <- Y[W == 0 & W_star_matrix[, b] == 1] + c_val
}
}
return(list(W_star = W_star_matrix, Y_star = Y_star_array))
}
# Grid Setup
c_grid <- seq(-5, 5, by = 0.1)
# Run Simulations Once
sim_results <- generate_simulations(Y, W, n, n_1, B, c_grid)
W_star_matrix <- sim_results$W_star
Y_star_array <- sim_results$Y_star
shares_exceeding_tau_hat <- numeric(length(c_grid))
# Iterate over c values
for (j in 1:length(c_grid)) {
tau_star_results <- numeric(B)  # Vector to store tau_star values
# Iterate over simulations
for (i in 1:B) {
tau_star_results[i] <- mean(Y_star_array[W_star_matrix[, i] == 1, i, j]) - mean(Y_star_array[W_star_matrix[, i] == 0, i, j])
}
shares_exceeding_tau_hat[j] <- mean(tau_star_results > tau_hat)
}
# Filter c Values with Share Between 0.025 and 0.975
valid_c_values <- c_grid[shares_exceeding_tau_hat < 0.975 & shares_exceeding_tau_hat > 0.025]
# Find and Print Confidence Interval
lower_bound <- min(valid_c_values)
upper_bound <- max(valid_c_values)
cat("95% Confidence Interval for Treatment Effect: [", lower_bound, ", ", upper_bound, "]\n")
set.seed(123)  # For reproducibility
# Parameters
n <- 1000  # Number of individuals
T <- 4     # Number of time periods
tau <- c(0.5, 0.3, -0.2)  # Treatment effects for k = 0, 1, 2
# Simulate adoption times
E <- sample(c(2, 3, 4, Inf), n, replace = TRUE, prob = c(1/4, 1/4, 1/4, 1/4))
# Function to generate Y_i,t
generate_Y <- function(n, T, E, tau) {
alpha <- rnorm(n)  # Individual effects
lambda <- rnorm(T)  # Time effects
Y <- matrix(NA, nrow = n, ncol = T)  # Storage for outcomes
for (i in 1:n) {
for (t in 1:T) {
treatment_effect <- sum(tau * (1:T == (t - E[i] + 1)))
Y[i, t] <- alpha[i] + lambda[t] + treatment_effect + rnorm(1, sd = 0.5)
}
}
return(Y)
}
# Generate data
Y <- generate_Y(n, T, E, tau)
# Prepare data for regression
data <- data.frame(
id = rep(1:n, each = T),
time = rep(1:T, n),
Y = as.vector(Y)
)
# Add dummy variables for event-time interactions
for (k in 0:2) {
data[paste0("D_", k)] <- as.numeric(data$time - E[data$id] == k)
}
# Run OLS regression with fixed effects
library(lmtest)
library(sandwich)
library(plm)
# Define panel data structure
pdata <- pdata.frame(data, index = c("id", "time"))
# Model: dependent on whether you want to include individual and time fixed effects
model <- plm(Y ~ D_0 + D_1 + D_2 + factor(time) + factor(id), data = pdata, model = "within")
# Display results
summary(model)
# Calculate robust standard errors
coeftest(model, vcov = vcovHC(model, type = "HC1"))
set.seed(123)  # For reproducibility
# Parameters
n <- 1000  # Number of individuals
T <- 4     # Number of time periods
tau <- c(0.5, 0.3, -0.2)  # Treatment effects for k = 0, 1, 2
# Simulate adoption times
E <- sample(c(2, 3, 4, Inf), n, replace = TRUE, prob = c(1/4, 1/4, 1/4, 1/4))
# Function to generate Y_i,t
generate_Y <- function(n, T, E, tau) {
alpha <- rnorm(n)  # Individual effects
lambda <- rnorm(T)  # Time effects
Y <- matrix(NA, nrow = n, ncol = T)  # Storage for outcomes
for (i in 1:n) {
for (t in 1:T) {
treatment_effect <- sum(tau * (1:T == (t - E[i] + 1)))
Y[i, t] <- alpha[i] + lambda[t] + treatment_effect + rnorm(1, sd = 0.5)
}
}
return(Y)
}
# Generate data
Y <- generate_Y(n, T, E, tau)
# Prepare data for regression
data <- data.frame(
id = rep(1:n, each = T),
time = rep(1:T, n),
Y = as.vector(Y)
)
# Add dummy variables for event-time interactions
for (k in 0:2) {
data[paste0("D_", k)] <- as.numeric(data$time - E[data$id] == k)
}
# Run OLS regression with fixed effects
library(lmtest)
library(sandwich)
library(plm)
# Define panel data structure
pdata <- pdata.frame(data, index = c("id", "time"))
# Model: dependent on whether you want to include individual and time fixed effects
model <- plm(Y ~ D_0 + D_1 + D_2 + factor(time) + factor(id), data = pdata, model = "within")
# Display results
summary(model)
# Calculate robust standard errors
coeftest(model, vcov = vcovHC(model, type = "HC1"))
# Assuming you have run the model and it's stored in 'model'
# Extracting coefficients for tau_k
coefficients <- summary(model)$coefficients
# Let's assume your tau coefficients are named 'D_0', 'D_1', 'D_2' in the output
# Extract and prepare data for plotting
tau_data <- data.frame(
Lag = c("D_0", "D_1", "D_2"),
Coefficient = coefficients[c("D_0", "D_1", "D_2"), "Estimate"],
SE = coefficients[c("D_0", "D_1", "D_2"), "Std. Error"]
)
# Load ggplot2 for plotting
library(ggplot2)
# Create the plot
tau_plot <- ggplot(tau_data, aes(x = Lag, y = Coefficient)) +
geom_point() +
geom_errorbar(aes(ymin = Coefficient - 1.96 * SE, ymax = Coefficient + 1.96 * SE), width = 0.1) +
labs(title = "Estimated Treatment Effects by Lag",
x = "Lag (k periods after adoption)",
y = "Estimated Coefficient (tau_k)") +
theme_minimal()
# Print the plot
print(tau_plot)
set.seed(123)  # For reproducibility
# Parameters
n <- 1000  # Number of individuals
T <- 4     # Number of time periods
tau <- c(0.5, 0.3, -0.2)  # Treatment effects for k = 0, 1, 2
# Simulate adoption times
E <- sample(c(2, 3, 4, Inf), n, replace = TRUE, prob = c(1/4, 1/4, 1/4, 1/4))
# Function to generate Y_i,t
generate_Y <- function(n, T, E, tau) {
alpha <- rnorm(n)  # Individual effects
lambda <- rnorm(T)  # Time effects
Y <- matrix(NA, nrow = n, ncol = T)  # Storage for outcomes
for (i in 1:n) {
for (t in 1:T) {
treatment_effect <- sum(tau * (1:T == (t - E[i] + 1)))
Y[i, t] <- alpha[i] + lambda[t] + treatment_effect + rnorm(1, sd = 0.5)
}
}
return(Y)
}
# Generate data
Y <- generate_Y(n, T, E, tau)
# Prepare data for regression
data <- data.frame(
id = rep(1:n, each = T),
time = rep(1:T, n),
Y = as.vector(Y)
)
# Add dummy variables for event-time interactions
for (k in 0:2) {
data[paste0("D_", k)] <- as.numeric(data$time - E[data$id] == k)
}
# Run OLS regression with fixed effects
library(lmtest)
library(sandwich)
library(plm)
# Define panel data structure
pdata <- pdata.frame(data, index = c("id", "time"))
# Model: dependent on whether you want to include individual and time fixed effects
model <- plm(Y ~ D_0 + D_1 + D_2 + factor(time) + factor(id), data = pdata, model = "within")
# Display results
summary(model)
# Calculate robust standard errors
coeftest(model, vcov = vcovHC(model, type = "HC1"))
# Assuming you have run the model and it's stored in 'model'
# Extracting coefficients for tau_k
coefficients <- summary(model)$coefficients
# Let's assume your tau coefficients are named 'D_0', 'D_1', 'D_2' in the output
# Extract and prepare data for plotting
tau_data <- data.frame(
Lag = c("D_0", "D_1", "D_2"),
Coefficient = coefficients[c("D_0", "D_1", "D_2"), "Estimate"],
SE = coefficients[c("D_0", "D_1", "D_2"), "Std. Error"]
)
# Load ggplot2 for plotting
library(ggplot2)
# Create the plot
tau_plot <- ggplot(tau_data, aes(x = Lag, y = Coefficient)) +
geom_point() +
geom_errorbar(aes(ymin = Coefficient - 1.96 * SE, ymax = Coefficient + 1.96 * SE), width = 0.1) +
labs(title = "Estimated Treatment Effects by Lag",
x = "Lag (k periods after adoption)",
y = "Estimated Coefficient (tau_k)") +
theme_minimal()
# Print the plot
print(tau_plot)
set.seed(123)  # For reproducibility
# Parameters
n <- 1000  # Number of individuals
T <- 4     # Number of time periods
tau <- c(0.5, 0.3, -0.2)  # Treatment effects for k = 0, 1, 2
# Simulate adoption times, using Inf to represent non-adoption within the observed time frame
E <- sample(c(2, 3, 4, Inf), n, replace = TRUE, prob = c(1/4, 1/4, 1/4, 1/4))
# Function to generate Y_i,t
generate_Y <- function(n, T, E, tau) {
alpha <- rnorm(n)  # Individual effects
lambda <- rnorm(T)  # Time effects
Y <- matrix(NA, nrow = n, ncol = T)  # Storage for outcomes
for (i in 1:n) {
for (t in 1:T) {
# Calculating treatment effect at the current time t for each individual
treatment_effect <- sum(tau * (1:T == (t - E[i] + 1)), na.rm = TRUE)
Y[i, t] <- alpha[i] + lambda[t] + treatment_effect + rnorm(1, sd = 0.5)
}
}
return(Y)
}
# Generate data
Y <- generate_Y(n, T, E, tau)
# Prepare data for regression
data <- data.frame(
id = rep(1:n, each = T),
time = rep(1:T, n),
Y = as.vector(Y),
E = rep(E, each = T)
)
# Add dummy variables for event-time interactions
for (k in 0:2) {
# Creating dummy variables for each lag: D_0 for τ_0, D_1 for τ_1, and D_2 for τ_2
data[paste0("D_", k)] <- as.numeric(data$time - data$E == k)
}
# Run OLS regression with fixed effects using plm
library(plm)
pdata <- pdata.frame(data, index = c("id", "time"))
model <- plm(Y ~ D_0 + D_1 + D_2 + factor(time), data = pdata, model = "within")
# Display results
summary(model)
# Calculate robust standard errors using coeftest from the lmtest package
library(lmtest)
library(sandwich)
coeftest(model, vcov = vcovHC(model, type = "HC1"))
# Extract coefficients for τ_k and prepare visualization
coefficients <- summary(model)$coefficients
coefficients_tau <- coefficients[grep("D_", rownames(coefficients)), ]
# Visualization with ggplot2
library(ggplot2)
tau_data <- data.frame(
Lag = rownames(coefficients_tau),
Coefficient = coefficients_tau[, "Estimate"],
SE = coefficients_tau[, "Std. Error"]
)
tau_plot <- ggplot(tau_data, aes(x = Lag, y = Coefficient)) +
geom_point() +
geom_errorbar(aes(ymin = Coefficient - 1.96 * SE, ymax = Coefficient + 1.96 * SE), width = 0.1) +
labs(title = "Estimated Treatment Effects by Lag",
x = "Lag (k periods after adoption)",
y = "Estimated Coefficient (tau_k)") +
theme_minimal()
print(tau_plot)
if (!require("devtools")) {
install.packages("devtools")
}
devtools::install_github("diegovalle/mxmaps")
mxmunicipio_choropleth(df_mxmunicipio_2020, num_colors = 1,
zoom = subset(df_mxmunicipio_2020, state_name %in%
c("Yucatán", "Veracruz"))$region,
title = "Percentage of the population that speaks\nan indigenous language in Yucatán and Veracruz",
show_states = FALSE,
legend = "%")
df_mxstate_2020$value <- df_mxstate_2020$pop
library("mxmaps")
mxmunicipio_choropleth(df_mxmunicipio_2020, num_colors = 1,
zoom = subset(df_mxmunicipio_2020, state_name %in%
c("Yucatán", "Veracruz"))$region,
title = "Percentage of the population that speaks\nan indigenous language in Yucatán and Veracruz",
show_states = FALSE,
legend = "%")
df_mxstate_2020$value <- df_mxstate_2020$pop
mxstate_choropleth(df_mxstate_2020,
title = "Total population, by state")
mxmunicipio_choropleth(df_mxmunicipio_2020, num_colors = 1,
zoom = subset(df_mxmunicipio_2020, state_name %in%
c("Yucatán", "Veracruz"))$region,
title = "Percentage of the population that speaks\nan indigenous language in Yucatán and Veracruz",
show_states = FALSE,
legend = "%")
mxmunicipio_choropleth(df_mxmunicipio_2020, num_colors = 1,
zoom = subset(df_mxmunicipio_2020, state_name %in%
c("Yucatán"))$region,
title = "Percentage of the population that speaks\nan indigenous language in Yucatán and Veracruz",
show_states = FALSE,
legend = "%")
mxmunicipio_choropleth(df_mxmunicipio_2020, num_colors = 1,
zoom = subset(df_mxmunicipio_2020, state_name %in%
c("Yucatán", "Veracruz"))$region,
title = "Percentage of the population that speaks\nan indigenous language in Yucatán and Veracruz",
show_states = FALSE,
legend = "%")
mxmunicipio_choropleth(df_mxmunicipio_2020, num_colors = 1,
zoom = subset(df_mxmunicipio_2020, metro_area %in%
c("Valle de México",
"Puebla-Tlaxcala",
"Cuernavaca",
"Toluca"))$region,
title = "Percentage of the population that speaks\nan indigenous language",
legend = "%")
?DoubleMLPL
# Assuming 'decomposition_results' is a data frame with columns:
# 'covariate' (names of covariates), 'contribution' (numeric contributions)
library(ggplot2)
# Example data
decomposition_results <- data.frame(
covariate = c("Education", "Experience", "Gender", "Region", "Industry"),
contribution = c(0.15, -0.05, 0.1, -0.02, 0.07)
)
# Plot
ggplot(decomposition_results, aes(x = reorder(covariate, contribution), y = contribution)) +
geom_bar(stat = "identity", fill = "skyblue") +
coord_flip() +  # Flip coordinates for easier reading
labs(title = "Fairlie Decomposition: Contribution of Covariates to Outcome Difference",
x = "Covariate",
y = "Contribution to Outcome Difference") +
theme_minimal()
library(randomForest)
library(pdp)
# Example data and model
data(boston, package = "MASS")
rf_model <- randomForest(medv ~ ., data = boston)
library(randomForest)
library(pdp)
library(MASS)
# Example data and model
data(boston, package = "MASS")
rf_model <- randomForest(medv ~ ., data = boston)
# Install and load the necessary packages if not already installed
if (!requireNamespace("mlbench", quietly = TRUE)) {
install.packages("mlbench")
}
# Load packages
library(randomForest)
library(pdp)
library(mlbench)
# Load the BostonHousing dataset
data("BostonHousing")
boston <- BostonHousing
# Fit a random forest model
rf_model <- randomForest(medv ~ ., data = boston)
# Create a PDP for the 'lstat' variable
pdp_plot <- partial(rf_model, pred.var = "lstat", plot = TRUE, rug = TRUE,
main = "Partial Dependence Plot for 'lstat'")
pdp_plot
##### Labor Econ Session 7 - The art of beautiful plots
### Author: Sevin Kaytan (following source https://www.charlesbordet.com/en/make-beautiful-charts-ggplot2/#)
## Date: 13 Nov 2023
#Install Packages
# install.packages("ggplot2")
# install.packages("data.table")
#Install libraries and define root folder
rm(list=ls())
# library(data.table)
# library(ggplot2)
rootFolder <-"//Users/sevinkaytan/Desktop/TA/beautiful_plots/"
# Define a list of libraries/packages required for the analysis
libs <- c(
"data.table","ggplot2"
)
# Identify libraries that are not currently installed
new_packages <- libs[!(libs %in% installed.packages()[, "Package"])]
# Install any missing libraries except 'unitdid'
if (length(new_packages)) {
install.packages(new_packages)
}
# Load the required libraries
lapply(libs, require, character.only = TRUE)
#Import data
pokemon <- fread(paste0(rootFolder,"/pokemonGO.csv")) # don't forget to change!
##### Labor Econ Session 7 - The art of beautiful plots
### Author: Sevin Kaytan (following source https://www.charlesbordet.com/en/make-beautiful-charts-ggplot2/#)
## Date: 13 Nov 2023
#Install Packages
# install.packages("ggplot2")
# install.packages("data.table")
#Install libraries and define root folder
rm(list=ls())
# library(data.table)
# library(ggplot2)
rootFolder <- "C:/Users/spammer/Downloads/beautiful_plots.zip/beautiful_plots"
# Define a list of libraries/packages required for the analysis
libs <- c(
"data.table","ggplot2"
)
# Identify libraries that are not currently installed
new_packages <- libs[!(libs %in% installed.packages()[, "Package"])]
# Install any missing libraries except 'unitdid'
if (length(new_packages)) {
install.packages(new_packages)
}
# Load the required libraries
lapply(libs, require, character.only = TRUE)
#Import data
pokemon <- fread(paste0(rootFolder,"/pokemonGO.csv")) # don't forget to change!
##### Labor Econ Session 7 - The art of beautiful plots
### Author: Sevin Kaytan (following source https://www.charlesbordet.com/en/make-beautiful-charts-ggplot2/#)
## Date: 13 Nov 2023
#Install Packages
# install.packages("ggplot2")
# install.packages("data.table")
#Install libraries and define root folder
rm(list=ls())
# library(data.table)
# library(ggplot2)
rootFolder <- "C:/Users/spammer/Downloads/beautiful_plots.zip/beautiful_plots/beautiful_plots"
# Define a list of libraries/packages required for the analysis
libs <- c(
"data.table","ggplot2"
)
# Identify libraries that are not currently installed
new_packages <- libs[!(libs %in% installed.packages()[, "Package"])]
# Install any missing libraries except 'unitdid'
if (length(new_packages)) {
install.packages(new_packages)
}
# Load the required libraries
lapply(libs, require, character.only = TRUE)
#Import data
pokemon <- fread(paste0(rootFolder,"/pokemonGO.csv")) # don't forget to change!
##### Labor Econ Session 7 - The art of beautiful plots
### Author: Sevin Kaytan (following source https://www.charlesbordet.com/en/make-beautiful-charts-ggplot2/#)
## Date: 13 Nov 2023
#Install Packages
# install.packages("ggplot2")
# install.packages("data.table")
#Install libraries and define root folder
rm(list=ls())
# library(data.table)
# library(ggplot2)
rootFolder <- "C:/Users/spammer/Downloads/beautiful_plots.zip/beautiful_plots/beautiful_plots"
# Define a list of libraries/packages required for the analysis
libs <- c(
"data.table","ggplot2"
)
# Identify libraries that are not currently installed
new_packages <- libs[!(libs %in% installed.packages()[, "Package"])]
# Install any missing libraries except 'unitdid'
if (length(new_packages)) {
install.packages(new_packages)
}
# Load the required libraries
lapply(libs, require, character.only = TRUE)
#Import data
pokemon <- fread(paste0(rootFolder,"./pokemonGO.csv")) # don't forget to change!
# Install necessary packages if not installed
if (!requireNamespace("sf", quietly = TRUE)) install.packages("sf")
if (!requireNamespace("rdrobust", quietly = TRUE)) install.packages("rdrobust")
if (!requireNamespace("ggplot2", quietly = TRUE)) install.packages("ggplot2")
# Load required libraries
library(sf)
library(rdrobust)
library(ggplot2)
# ----------------------------------------------------------------------
# 1. Load Data
# ----------------------------------------------------------------------
data_path <- "../../bld/data/merged_national_with_hre_clipped.gpkg"
# Load dataset using sf (spatial data format)
map_data <- st_read(data_path)
# Install necessary packages if not installed
if (!requireNamespace("sf", quietly = TRUE)) install.packages("sf")
if (!requireNamespace("rdrobust", quietly = TRUE)) install.packages("rdrobust")
if (!requireNamespace("ggplot2", quietly = TRUE)) install.packages("ggplot2")
# Load required libraries
library(sf)
library(rdrobust)
library(ggplot2)
# Set working directory to script location (Modify as needed)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# ----------------------------------------------------------------------
# 1. Load Data
# ----------------------------------------------------------------------
data_path <- "../../bld/data/merged_national_with_hre_clipped.gpkg"
# Load dataset using sf (spatial data format)
map_data <- st_read(data_path)
# ----------------------------------------------------------------------
# 2. Define Variables for Fuzzy RDD
# ----------------------------------------------------------------------
# Running variable: Distance to Catholic HRE border
map_data$running_var <- map_data$dist_to_catholic_border  # Ensure this column exists
# Treatment variable: Overlap with Catholic HRE (continuous treatment for fuzzy RDD)
map_data$treatment <- map_data$hre_catholic_overlap_pct  # Between 0 and 1
# Outcome variable: Far-right vote share
map_data$outcome <- map_data$far_right
# ----------------------------------------------------------------------
# 3. Perform Fuzzy RDD using rdrobust
# ----------------------------------------------------------------------
# Run fuzzy RDD
fuzzy_rdd <- rdrobust(
y = map_data$outcome,         # Outcome variable
x = map_data$running_var,     # Running variable (distance to border)
fuzzy = map_data$treatment,   # Fuzzy treatment (overlap with Catholic HRE)
kernel = "triangular",        # Triangular kernel for local fitting
bwselect = "mserd"            # Bandwidth selection rule
)
